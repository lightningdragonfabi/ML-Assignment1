{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install torch if necessary\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a4df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Gomoku environment\n",
    "class GomokuEnv:\n",
    "    def __init__(self, size=8):\n",
    "        \"\"\"Initialize the Gomoku environment with an 8x8 board.\"\"\"\n",
    "        self.size = size\n",
    "        self.board = np.zeros((size, size), dtype=int)\n",
    "        self.current_player = 1\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the board and game state.\"\"\"\n",
    "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
    "        self.current_player = 1\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.board.copy()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute a move for the current player. Returns board, reward, is_done, info\"\"\"\n",
    "        if self.done:\n",
    "            raise Exception(\"Game is over\")\n",
    "        row, col = divmod(action, self.size)\n",
    "        if self.board[row, col] != 0:\n",
    "            raise Exception(\"Invalid move\")\n",
    "        self.board[row, col] = self.current_player\n",
    "        if self._check_winner(row, col):\n",
    "            self.done = True\n",
    "            self.winner = self.current_player\n",
    "            return self.board.copy(), 1, True, f'player {self.current_player} wins'\n",
    "        if np.all(self.board != 0):\n",
    "            self.done = True\n",
    "            return self.board.copy(), 0, True, 'draw'\n",
    "        self.current_player *= -1\n",
    "        return self.board.copy(), 0, False, ''\n",
    "\n",
    "    def _check_winner(self, row, col):\n",
    "        \"\"\"Check if the move at (row, col) wins the game.\"\"\"\n",
    "        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]\n",
    "        player = self.board[row, col]\n",
    "        for dr, dc in directions:\n",
    "            count = 1\n",
    "            for i in range(1, 5):\n",
    "                r, c = row + dr * i, col + dc * i\n",
    "                if not (0 <= r < self.size and 0 <= c < self.size) or self.board[r, c] != player:\n",
    "                    break\n",
    "                count += 1\n",
    "            for i in range(1, 5):\n",
    "                r, c = row - dr * i, col - dc * i\n",
    "                if not (0 <= r < self.size and 0 <= c < self.size) or self.board[r, c] != player:\n",
    "                    break\n",
    "                count += 1\n",
    "            if count >= 5:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the current board state.\"\"\"\n",
    "        print('   ' + ' '.join(str(i) for i in range(self.size)))\n",
    "        logging.info('   ' + ' '.join(str(i) for i in range(self.size)))\n",
    "        print('  +' + '-' * (self.size * 2 - 1) + '+')\n",
    "        logging.info('  +' + '-' * (self.size * 2 - 1) + '+')\n",
    "        for i in range(self.size):\n",
    "            print(f'{i} |' + ' '.join(['X' if x == 1 else 'O' if x == -1 else '.' for x in self.board[i]]) + '|')\n",
    "            logging.info(f'{i} |' + ' '.join(['X' if x == 1 else 'O' if x == -1 else '.' for x in self.board[i]]) + '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, env, device, learning_rate=0.0001, gamma=0.95, epsilon=1.0, epsilon_decay=0.999, epsilon_min=0.05):\n",
    "        \"\"\"Initialize the DQN agent with hyperparameters.\"\"\"\n",
    "        self.state_size = state_size  # (2, size, size)\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.device = torch.device(device)\n",
    "        self.model = self._build_model().to(self.device)\n",
    "        self.target_model = deepcopy(self.model).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.env = env\n",
    "        self.update_target_freq = 100\n",
    "        self.step_counter = 0\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Build the CNN architecture for Q-value approximation.\"\"\"\n",
    "        model = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * self.state_size[1] * self.state_size[2], 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.action_size)\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def get_state(self, board, current_player):\n",
    "        \"\"\"Convert board to two-channel state (current player's pieces, opponent's pieces).\"\"\"\n",
    "        player_pieces = (board == current_player).astype(np.float32)\n",
    "        opponent_pieces = (board == -current_player).astype(np.float32)\n",
    "        return np.stack([player_pieces, opponent_pieces], axis=0)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
    "        available_actions = np.where(state[0, 0].flatten() + state[0, 1].flatten() == 0)[0]\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(available_actions)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        act_values = self.model(state_tensor).cpu().detach().numpy()[0]\n",
    "        act_values[state[0, 0].flatten() + state[0, 1].flatten() != 0] = float('-inf')\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def replay(self, batch_size=64):\n",
    "        \"\"\"Train the model using a batch of experiences.\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        self.step_counter += 1\n",
    "        if self.step_counter % self.update_target_freq == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        states = np.array([s for (s, a, r, ns, d) in minibatch])\n",
    "        next_states = np.array([ns for (s, a, r, ns, d) in minibatch])\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
    "        actions = torch.tensor([a for (s, a, r, ns, d) in minibatch], dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor([r for (s, a, r, ns, d) in minibatch], dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor([d for (s, a, r, ns, d) in minibatch], dtype=torch.float32).to(self.device)\n",
    "\n",
    "        q_values = self.model(states)\n",
    "        next_q_values = self.target_model(next_states).detach()\n",
    "        max_next_q = next_q_values.max(dim=1)[0]\n",
    "        # next_states is oppoent's state so we need to reverse the reward by multiply a negative number\n",
    "        targets = rewards + self.gamma * max_next_q * (1 - dones) * -0.9\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        loss = F.mse_loss(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.001)  # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "        logging.info(f'loss={loss.item()}')\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        \"\"\"Save the model weights.\"\"\"\n",
    "        torch.save(self.model.state_dict(), filename)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        \"\"\"Load the model weights if available.\"\"\"\n",
    "        if os.path.exists(filename):\n",
    "            self.model.load_state_dict(torch.load(filename))\n",
    "            self.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(agent, env, num_games=100):\n",
    "    \"\"\"Evaluate the agent against a random opponent for 100 times. Returns the win rate of the agent\"\"\"\n",
    "    wins = 0\n",
    "    for _ in range(num_games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == 1:\n",
    "                state_channels = agent.get_state(state, 1)\n",
    "                state_channels = np.reshape(state_channels, [1, 2, env.size, env.size])\n",
    "                action = agent.act(state_channels)\n",
    "            else:\n",
    "                available_actions = np.where(state.flatten() == 0)[0]\n",
    "                action = np.random.choice(available_actions)\n",
    "            state, _, done, info = env.step(action)\n",
    "            if done and env.winner == 1:\n",
    "                wins += 1\n",
    "    return wins / num_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_dqn(episodes=10000000000, batch_size=64, model_filename='gomoku_dqn.pth'):\n",
    "    \"\"\"Train the DQN agent using self-play.\"\"\"\n",
    "    env = GomokuEnv(size=8)\n",
    "    best_win_rate = 0\n",
    "    state_size = (2, env.size, env.size)\n",
    "    action_size = env.size ** 2\n",
    "    agent = DQNAgent(state_size, action_size, env, \"cpu\")\n",
    "    agent.load_model(model_filename)\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        # Initial random move by player 1\n",
    "        available_actions = np.where(state.flatten() == 0)[0]\n",
    "        action = np.random.choice(available_actions)\n",
    "        state, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            continue\n",
    "        for time in range(8**2 - 1):\n",
    "            current_player = env.current_player\n",
    "            state_channels = agent.get_state(state, current_player)\n",
    "            state_channels_reshaped = np.reshape(state_channels, [1, 2, env.size, env.size])\n",
    "            action = agent.act(state_channels_reshaped)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_current_player = env.current_player\n",
    "\n",
    "            # If a player wins, the other loses. So we need to adjust last reward to a negative number to punish the loser.\n",
    "            if done and env.winner is not None:\n",
    "                # agent.memory[-1][2] = -1\n",
    "                agent.memory[-1] = agent.memory[-1][0], agent.memory[-1][1], -2, agent.memory[-1][3], agent.memory[-1][4]\n",
    "\n",
    "            next_state_channels = agent.get_state(next_state, next_current_player)\n",
    "            next_state_channels_reshaped = np.reshape(next_state_channels, [1, 2, env.size, env.size])\n",
    "            agent.remember(state_channels, action, reward, next_state_channels, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                logging.info(f\"episode: {e}/{episodes}, time: {time}, e: {agent.epsilon}, info: {info}\")\n",
    "                break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        if e % 1000 == 0:\n",
    "            win_rate = evaluate(agent, env)\n",
    "            if win_rate >= best_win_rate:\n",
    "                logging.info(f\"Episode {e}, Win rate vs random: {win_rate}\")\n",
    "                print(f\"Episode {e}, Win rate vs random: {win_rate}\")\n",
    "                agent.save_model(model_filename)\n",
    "                env.render()\n",
    "                best_win_rate = win_rate if win_rate != 1 else 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(filename='gomoku_dqn1.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    train_dqn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
